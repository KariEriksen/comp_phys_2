\section{Theory}

\subsection{Ansatz for the wavefunction and the hamiltonian}

We are studying a system of two electrons confined in a harmonic oscillator trap described by the Hamiltonian 

\begin{equation}\label{eq:Hamilt}
\hat{H} = \sum_{i=1}^N \left( - \frac{1}{2} \nabla_i^2 + \frac{1}{2} \omega^2 r_i^2 \right) + \sum_{i<j} \frac{1}{r_{ij}}
\end{equation}

where the first sum is the standard harmonic oscillator part and the last is the interacting part between the electrons and $N$ represent the number of particles. $\omega$ is the oscillator frequency of the trap and $r_i$ is the position of particle $i$, whereas $r_{ij}$ is the distance between the particles and given as $r_{ij} = |\mathbf{r_i} - \mathbf{r_j}|$. \\
Our wave function is given from the energy of the restricted Boltzmann machine, see \eqref{eq:E_rbm}, which is the joint energy functional between the visible and hidden nodes. From the marginal probability of the joint probability distribution, see \eqref{eq:F_rbm_marg}, we get our wave equation.                     

\begin{align}\label{eq:F_rbm}
\Psi(X) &= F_{rbm}(X) \\
&= \frac{1}{Z} \exp \left( -\sum_{i}^{M} \frac{(X_i - a_i)^2}{2 \sigma^2} \right) \prod_{j}^{N} \left( 1 + \exp \left( b_j + \sum_{i}^{M} \frac{X_i \omega_{ij}}{\sigma^2} \right) \right)
\end{align}

Here $Z$ is the partition function, $X_i$ represents the visible nodes running up to $M$, and $a_i$ and $b_j$ are the biases described in the section below, \eqref{sec:rbm}, where number of hidden nodes $j$ runs up to $N$. $\omega_{ij}$ is an $M \times N$ matrix holding the weights connecting the visible nodes with the hidden and $\sigma$ is the standard deviation of the noise in our model. \\ 

\subsection{Variational Monte Carlo}

Monte Carlo simulations are widely used methods in numerical science that employs random walkers. 
In this project we are taking a closer look at trapped bosons. We are given a trail wave function we assume is as close to the real case as possible, $\Psi_T(\mathbf{R};\alpha)$ where $\mathbf{R} = (\mathbf{R}_1, ... , \mathbf{R}_N)$ is the position of the different particles. 
From quantum mechanics we know the probability distribution is given by the wave function. 

$$P(\mathbf{R}; \alpha) = \frac{|\Psi_T(\mathbf{R};\alpha)|^2}{\int |\Psi_T(\mathbf{R};\alpha)|^2 d\mathbf{R}}$$

Monte Carlo integration allow us to evaluate the integral at hand. The expectation value of the Hamiltonian is given as follows. 

$$\langle \widehat{\mathbf{H}}\rangle = \frac{\int d \mathbf{R} \Psi^{\ast} (\mathbf{R})H(\mathbf{R}) \Psi(\mathbf{R})}{\int d \mathbf{R} \Psi^{\ast} (\mathbf{R}) \Psi(\mathbf{R})}$$

The variational principle states that the expectation value of the Hamiltonian is an upper-bound for the ground state energy of the Hamiltonian.

$$E_0 \leq \langle H \rangle$$

This is what the Variational Monte Carlo method bases itself on. Given a probability distribution we can evaluate the wave function and look for a local minimum. We define the local energy by

$$\widehat{\mathbf{E}}_L(\mathbf{R};\alpha) = \frac{1}{\Psi_T(\mathbf{R};\alpha)}\widehat{\mathbf{H}}\Psi_T(\mathbf{R};\alpha).$$

Then the expectation value of the local energy is given by

$$\langle \widehat{\mathbf{E}}_L\rangle = \int P(\mathbf{R}) \widehat{\mathbf{E}}_L d\mathbf{R} \approx \frac{1}{N} \sum_{i = 1}^N \mathbf{E_L}(x_i)$$

where $N$ is the number of Monte Carlo cycles. Now we can calculate the probability distribution and the local energy. And for each cycle we propose a new configuration $\mathbf{R}_p$ for the system and hopefully we come closer to the ground state.

$$\mathbf{R}_p = \mathbf{R} + r \ast \Delta \mathbf{R}$$

\subsection{Restricted Boltzmann Machine}\label{sec:rbm}

Restricted Boltzmann machine is a energy-based generative model which include hidden and visible variables [3]. It consists of a two-layer network with a two dimensional matrix $W_{ij}$ telling how strong the connections between the hidden and visible nodes are. The energy related to a configuration of the nodes is what forms the basis of our model. It is given in \eqref{eq:E_rbm}.

\begin{equation}\label{eq:E_rbm}
E(\mathbf{v}, \mathbf{h}) = - \sum_{i} a_i(v_i) - \sum_{\mu} b_j(h_j) - \sum_{ij} W_{ij}v_i h_j 
\end{equation}

Here $a_i(v_i)$ and $b_j(h_j)$ are bias functions of the visible and hidden layers which we are allowed to choose our self. In our case we want the visible layer to take a continuous form and the hidden layer to be binary, (Gaussian-binary), meaning $a_i$ and $b_j$ takes the following from

$$a_i(v_i) = \frac{v_i^2}{2 \sigma_i^2}, \ \ \ b_j(h_j)= b_jh_j.$$

We are working with restricted Boltzmann machine meaning there is no connection between nodes within layers, only between layers. Also this network is a generative one so we want our network to learn a probability distribution. We begin with the joint probability distribution of the visible and hidden nodes is given in \eqref{eq:F_rbm_joint} where $Z$ is the partition function, see \eqref{eq:Z}. 

\begin{equation}\label{eq:F_rbm_joint}
F_{rbm} (\mathbf{X}, \mathbf{h}) = \frac{1}{Z} e^{-E(\mathbf{X}, \mathbf{h})}
\end{equation}

\begin{equation}\label{eq:Z}
Z = \int \int \frac{1}{Z} e^{-E(\mathbf{X}, \mathbf{h})} d\mathbf{x}d\mathbf{h}
\end{equation}

From \eqref{eq:F_rbm_joint} we can marginalize over all the hidden units and get the distribution over the visible units. And as mentioned above this is what we use to represent our wave function. 

\begin{align}\label{eq:F_rbm_marg}
F_{rbm} (\mathbf{X}) &= \sum_{\mathbf{h}} F_{rbm} (\mathbf{X}, \mathbf{h}) \\
&= \frac{1}{Z} \sum_{\mathbf{h}} e^{-E(\mathbf{X}, \mathbf{h})}
\end{align}

Since we have no data to train we use the technique of reinforcement learning. We feed the network with input using Monte Carlo method and based on the variational principle we find the ground state of the system by seeking the configuration that gives the lowest quantum mechanical energy. According to this principle we change the weights and biases by gradient descent method and hopefully the network will converge towards the correct state. 

\subsection{The Metropolis Algorithm}

How we select the new configurations during the simulation is given by the Metropolis Algorithm. 
Ideally we would have a transition probability matrix which told us how likely it would be for each particle to move in the configuration space. Since we don't we try and model it.
We define the following entities:
\\
\\
$P_i^{(n)} \rightarrow$ probability of finding the system in state $i$ at the n'th step.
\\
$j \rightarrow$ possible new step
\\
$A_{i \rightarrow j} \rightarrow$ probability of acceptance. 
\\
$T_{j \rightarrow i} \rightarrow$ probability of making the transition  
\\
\\
Now we can say that a transition probability matrix can be constructed by $T_{j \rightarrow i}A_{j \rightarrow i}.$ The probability of finding the system in state $i$ at step $n$ is

\begin{equation}\label{eq:prob}
P_i^{(n)} = \sum_j \left[P_j^{(n-1)} T_{j \rightarrow i}A_{j \rightarrow i} + P_j^{(n-1)}T_{i \rightarrow j}(1 - A_{i \rightarrow j})\right].
\end{equation}

We want to push the system towards high density regional space of $P_i^{(n)}$. /We want to select states according to the probability distribution. In that way to converge to the desired stationary distribution $p_i$.

$$P_i^{(n \rightarrow \infty)} \rightarrow p_i$$

Using this statement and the fact that the sum over all possible transitional probabilities is one we can rewrite the above equation \ref{eq:prob}.

$$\sum_j [p_j T_{j \rightarrow i}A_{j \rightarrow i} - p_i T_{i \rightarrow j}A_{i \rightarrow j}] = 0$$

In order to stop at the configuration where we have reached equilibrium we us the condition of detailed balance, and it gives us 

\begin{algorithm}
\KwData{matrix $\mathbf{R}$}
\KwResult{float $\mathbf{E}_L$}
number of MC-cycles $N$\;
initialize $\mathbf{R}$\;
calculate $|\Psi_T(\mathbf{R})|^2$\;
\For{i in $[0, N]$}{
    $\mathbf{R}_p = \mathbf{R} + r \ast \Delta \mathbf{R}$\;
    calculate $\omega = \frac{|\Psi_T(\mathbf{R}_p)|^2}{|\Psi_T(\mathbf{R})|^2}$\;
    \eIf{$q \leq \omega$}{
        accept new move\;
    }
    {reject new move\;
    }
    update energy, $\mathbf{E}_L$\;
}
 \caption{Monte Carlo with Metropolis-Hastings}\label{alg:metro}
\end{algorithm}

$$\frac{A_{j \rightarrow i}}{A_{i \rightarrow j}} = \frac{p_i T_{i \rightarrow j}}{p_j T_{j \rightarrow i}}.$$

We choose to accept when the acceptance is bigger than 1. 

$$A_{j \rightarrow i} = \mathrm{min} \left( 1, \frac{p_i T_{i \rightarrow j}}{p_j T_{j \rightarrow i}}\right)$$

What we must calculate is equation \ref{eq:ratio}. And an example of the code is given further down, in Algorithm \ref{alg:metro}

\begin{equation}\label{eq:ratio}
\frac{p_i T_{i \rightarrow j}}{p_j T_{j \rightarrow i}} = \frac{|\Psi_T(\mathbf{R}_p)|^2}{|\Psi_T(\mathbf{R})|^2}
\end{equation}

\subsection{Importance Sampling}

Now we move to an extension of the Metropolis algorithm, the Importance sampling. This method provides us with a better way of suggesting new moves. The expression for the new move, $y$, is given by the solution of the Langevin equation. It reads 

$$y = x + D F(x) \Delta t + \xi \sqrt{\Delta t},$$

where $\xi$ is a gaussian random variable, $D$ is the diffusion coefficient and is $\frac{1}{2}$ and $\Delta t$ is the time step which take values between $[0.001, 0.01]$.
$F(x)$ is the drift force and is given by the gradient of the wave function. 

$$F = 2 \frac{1}{\Psi_T} \nabla \Psi_T$$

It is the drift force that ensures us we move particles towards regions of configuration space where the trail wave function is large. This method increases the efficiency of our program, since the standard Metropolis can suggest new moves in every direction with same probability. 
From the solution of the Fokker-Planck equation we get a new transition probability, the Green's function. 

$$G(y, x, \Delta t) = \frac{1}{(4 \pi D \Delta t)^{3N/2}} \exp (-(y - x - D F(x) \Delta t)^2/4 D \Delta t)$$

Now equation \ref{eq:ratio} from Metropolis algorithm changes to 

$$\frac{p_i T_{i \rightarrow j}}{p_j T_{j \rightarrow i}} = \frac{G(x, y, \Delta t)|\Psi_T(y)|^2}{G(y, x, \Delta t)|\Psi_T(x)|^2},$$

and we must calculate the drift force in order to find new moves and the Green's function to accept/reject these moves.

\subsection{Gibbs Sampling}

A special case of the Metropolis sampling is when we know the wave function to be positive definite. We can then express the wave function as in \eqref{eq:F_rbm_sq} and since we do not need to worry about a complex wave function we can model the probability density as we wish. Now we sample new configurations from the conditional probabilities directly. This we can do since all nodes in each layer are independent from one another. 

\begin{equation}\label{eq:hidden}
P(H_j= 1|\mathbf{x}) = \frac{1}{1 + e^{- b_j - \sum_i\frac{x_i w_{ij}}{\sigma^2}}}
\end{equation}

We first calculate values for the hidden nodes from the current configuration, which in the beginning is initialized with random numbers. Here we recognize the Sigmoid function. From this we begin generating new samples for the visible values. These however are picked from a normal distribution with mean equal $a_i + \sum_j \mathbf{w}_{ij} \mathbf{h}$ and variance equal $\sigma^2$.

\begin{equation}\label{eq:visible}
P(X_i|\mathbf{h}) = \mathcal{N}(X_i;a_i + \sum_j \mathbf{w}_{ij} \mathbf{h}, \sigma^2)
\end{equation}

The acceptance rate is 1, so the configuration is change every time. We then update the energy as before. Accept now we operate with a different wave function so the expression for both the energy and the derivatives in the gradient descent method are different, with a factor 0.5. So this must be accounted for. When evaluating the wave function we take a ratio and this factor vanishes, so changing the wave function itself is unnecessary.


\subsection{Gradient Descent}

Like most problems in physics the variational quantum problem is one of optimization. In the last project we optimizied the expectation value of the energy over the single parameter $\alpha$. Though in general interesting problems in many-body problems will not have one, but very many parameters to optimize. This is part of the motivation for the RBM implementation. That while the number of parameters are significantly larger than one, it may still be preferable to a more traditional ansatz for the wave function. In this project we then seek to optimize over the RBM parameters $\alpha = \{\mathbf{a}, \mathbf{b}, \mathbf{w}\}$. Put more formally we seek to minimize the expecattion of the energy over the aforementioned variational parameters by performing gradient descent. More sophisticated methods for optimization are available but will not be explored here. We're then left with the simple GD algorithm as before.

\begin{equation}
\alpha_{i+1} = \alpha_i -\gamma \frac{\partial \langle E_L (\alpha_i) \rangle}{\partial \alpha_i}
\end{equation}

The differentiation to find the expecation values are a bit more laborious now than for the previous project , but they are listed in appendix \ref{sec:GD}

\subsection{Blocking Method}

For the statistical analysis and error estimates we have used the technique of blocking. \
We are looking for the expectation value of the ground state energy of our system. In order to say something about how accurate these results are we want to look at the standard deviation. 

If our samples where uncorrelated we could calculate the standard deviation through the following equation 
$$\sigma = \sqrt{\frac{1}{n - 1} \left(\langle E_L^2\rangle - \langle E_L \rangle^2\right)}.$$

It can be showed that for correlated samples the equation for the standard deviation is
$$\sigma = \sqrt{\frac{1 + 2 \tau / \Delta t}{n - 1} \left(\langle E_L^2\rangle - \langle E_L \rangle^2\right)}$$

where $\tau$ is the time between one sample and the next uncorrelated sample and $\Delta t$ is the time between each sample.\
If we knew what $\tau$ was we could simply find our $\sigma$, but since we don't we use blocking to find it. The method is simple enough, we divide our samples of data into blocks and calculating the mean of each block. So if we have an amount of samples of $\langle E_L \rangle$, we divide these into $M$ blocks and calculate the mean. By plotting the standard deviation as function of the block size we can keep blocking and blocking until we see that the std. dev. stops increasing, and this is where the blocks are uncorrelated. Now we have an estimate for $\tau$ and thus the standard deviation.


